---
title: "Exacloud Training Workshop"
author: Ted Laderas
output:
  html_document:
    toc: true
    toc_depth: 3  # upto three depths of headings (specified by #, ## and ###)
    number_sections: true  ## if you want number sections at each table heiader
    theme: united  # many options for theme, this one is my favorite.
    highlight: tango  # specifies the syntax highlighting style
---

# Workshop Expectations

We want to foster a positive learning environment in this workshop. We expect everyone to adhere to the [Code of Conduct](http://github.com/laderast/exacloud_tutorial/CODE_OF_CONDUCT.md). We will enforce this and ask you to leave if you are not respectful to others.  
In short:


  + be respectful of each other's learning styles 
  + don't be dismissive or mean to someone who knows less than you
  + try to help people if you see them struggle and you can help. 

Additionally, please work together! The people in my workshops who have a really bad time and don't get anything out of it are the ones who try to do it alone. To quote Legend of Zelda, "*It's dangerous to go alone*". Talking through the material with someone will help you understand it.

We also are giving people post-it notes. Put them on your laptops so we can identify you if you need help or not. *Green* means "I'm okay, don't bug me", *Red* means "I need some help!".

# Prerequisites for this training:

You will need the following:

1) ACC Login
2) Permission to join exacloud
3) SSH terminal program (such as Terminal (Mac/Linux) or PuTTY(Windows))
4) Optional: an FTP program (WinSCP/Cyberduck) for transferring files

You should be able to understand the following:

1) What a shell script is
    + How to run a shell script
    + What a shebang is and setting up your shell script
2) Basic shell commands, including:
    + directory and file manipulation: `ls`, `rm`, `mkdir`, `rmdir`
    + file editing using `nano`, `vim`, or `emacs`.
    + how to set basic file permissions
    + what process monitoring is: `ps` and `kill`
3) How to run your program/workflow on the command line
    + R: `Rscript`
    + Python: `python`
    + Executable: `GATK`
4) Know how to find if your executable is already on exacloud
    + `which` and `where`

If you are unsure what any of these are or need a refresher, here is a DataCamp course: [Intro to Shell For Data Science]( https://www.datacamp.com/courses/introduction-to-shell-for-data-science). I recommend reviewing Chapters 1, 2, and 4.

## What is Exacloud?

[Exacloud](http://www.ohsu.edu/xd/research/research-cores/advanced-computing-center/exacloud.cfm) is OHSU's cluster computing environment. It's available for those people who have an ACC account and who request access. It exists to do many kinds of compute jobs:

+ Running aligners, such as Genome Analysis Toolkit (GATK)
+ Running complicated analysis jobs in R
+ General python processing (numpy, pandas, text analysis)
+ image analysis pipelines

The initial hardware was donated by Intel, but it also funded by the OHSU Knight Cancer Institute, Howard Hughes Medical Institute (HHMI), and the OHSU Center for Spatial Systems Biomedicine (OCSSB).

To run jobs effectively on `exacloud`, you must understand some basic shell scripting techniques and how exacloud is organized. That's the goal of this training workshop.

## Architecture of `exacloud`

Exacloud is organized by what are called nodes. Each node can be thought of as a multi-core or multi-CPU unit with 24 cores that share memory together. There are 250 nodes, which means that exacloud has has over 6000 CPUs of varying capabilities and memory that can be utilized by users who want to run parallel computing jobs. 

There are two different types of nodes on exacloud: the first type is the *head node*, which is the node that you sign into initially into exacloud (this is usually `exahead1.ohsu.edu`). The head node handles all of the scheduling and file transfer to the other nodes in the system. 

The other nodes are known as subservient *compute nodes*. They don't do anything unless the head node assigns them a task. Note that different nodes may have different configurations; they may have graphical processing units (GPUs) for computation, they may have less memory, or they might have special software installed on them (such as Docker). 

How does the head node tell the compute nodes what to do? There is a program called Slurm (Simple Linux Utility for Resource Management) installed on the *head node* that schedules computing jobs for all of the *compute nodes* on exacloud. How does it decide which jobs to run on which nodes? Part of it has to do with compute requirements - each individual job requires a certain amount of CPU power and Memory. You will request this when you run the job. Your request for allocation is balanced with the current load (i.e. other jobs currently running) on the cluster.

A note: `slurm` is different than the previous job scheduler on `exacloud`: HTCondor. Slurm does less than HTCondor: it really only lets you request allocations with particular memory/CPU requirements. It doesn't try to do more than that, unlike HTCondor, which provided a simple way to batch process files. This means that the majority of your scripts for running jobs will use `bash` scripting to get things done. 

## Useful reference material

Here's a [list of *terminology* and a list of *useful slurm commands*](reference/). If you get confused by any terms, please refer to it.

## How do I access `exacloud`?

Most of the time, you will sign in to `exahead1`. You will need a terminal program (Mac/Linux has terminal built in, Windows will need to [download something like PuTTY]()). There are two head nodes on exacloud:

  + `exahead1.ohsu.edu` (SSH in) - only accessible within OHSU Firewall
  + `exahead2.ohsu.edu` (SSH in) - only accessible within OHSU Firewall
  
If you need to transfer files, you will need to SFTP into here:
  
  + `exacloud.ohsu.edu` (access file system via FTP, for transferring files)

All of you should have external access to `exacloud`, which means you should be able to access it from off-campus. How to access it from off-campus? You will need to go through the ACC gateway first, and then ssh into `exahead1` or `exahead2`. 

+ `acc.ohsu.edu` : SSH gateway
+ `acc.ohsu.edu`: SFTP gateway (limited to 10 Gb)

## Filesystems 1: Your ACC directory

For reference: [The full list of file systems and storage available on exacloud](https://accdoc.ohsu.edu/exacloud/guide/storage/).

When you sign into `exahead1`, you will sign in to your ACC home directory. This directory is the directory that is accessible to all machines managed within ACC (for example, state.ohsu.edu, the DMICE server). I usually use my ACC directory for things like file libraries, my scripts, and such. Today we'll also use it for our data.

Note there's a limit on file storage for your acc directory (I believe it is 10 Gb), so if you need more room for your data, you will need to put it on lustre or the other options.

## Filesystems 2: Lustre

Lustre is the shared file system on exacloud. It is a distributed file system, that means that all nodes can easily access it. It is meant to be a *temporary* home for data and files that need to be processed by exacloud.

We will not be using lustre for our workshop today. If you want to put files on lustre, you will need to request access for a particular lab and group. If you are not attached to a group, you will not have access. 

If you don't need to process your data immediately, please don't leave it on lustre, as it's a shared resource. If you need to store your data for a longer period of time, please request [Research Data Storage](http://www.ohsu.edu/xd/research/research-cores/advanced-computing-center/research-data-storage.cfm) to store it.

## Filesystems 3: Research Data Storage

What if you're accumulating data, but don't need to analyse it right away? Then you should put it in [Research Data Storage](http://www.ohsu.edu/xd/research/research-cores/advanced-computing-center/research-data-storage.cfm) (RDS). Yearly costs are per terabyte of data. It's pretty reasonable, and the costs are on the ACC website.

RDS can also be used as a data backup for large datasets. Contact [email:acc@ohsu.edu](Advanced Computing Center) for more information.

# Workshop 

Now that we've got all that out of the way, we can actually start to do stuff on exacloud!

## Task 0 - Sign in to exacloud and clone this repo

1. To connect with exacloud, use the ssh command and input your password when prompted.

```
ssh USERNAME@exahead1.ohsu.edu
```

2. Now that you're signed in, clone the repository using `git`:

```
git clone https://github.com/laderast/exacloud_tutorial
```

3. You should now have a directory in your home directory called `exacloud_tutorial`. Change into the repository directory and you should be ready to go!

```
cd exacloud_tutorial
```

## Task 1 - Let's look at the overall structure of exacloud

Before we even do something on exacloud, we're going to look at how exacloud is organized.

1. Try running the following command. What partitions exist on exacloud?

```
scontrol show partition
```
That's a lot of information! Let's just focus on the partition names. There are five main partitions of compute nodes that you should have access to: `exacloud`, `highio`, `light`, `long_jobs`, and `very_long_jobs`. 

If you want to see the other partitions (including those you don't currently have access to), you can use the `-a` flag. Currently, there are a couple of partitions not accessible to everyone: `gpu`, and `mpi`. If you need to use these, you can request access via ACC.

```
scontrol show partition -a
```
The default partition (if you don't request one) is `exacloud`, which is where most of the general purpose nodes are. If your job needs to run a long time (longer than a week), you will need to specify using the `very_long_jobs` queue.

2. Let's look at information for one node. Take a look at the information you get. In particular, look at `CPUTot` (the total number of CPUs), `RealMemory` (amount of memory available to node in Mbs), and `Partitions`.

```
scontrol show node exanode-0-0
```

If you want to see all of the information for all of the nodes (again, use spacebar to page, and q to quit):

```
scontrol show node | less
```

3. Let's look at everyone who's running a job on exacloud. Try running this command. You can page through with the spacebar, and when you're done, you can use `q` to get out of `less`.

```
squeue | less
```

4. If you want to look at the jobs that a particular user is running, you can use the `-u` flag. Try `squeue -u` on one of the users in the queue, for example `wooma`:

```
squeue -u wooma
```
You will usually use `squeue -u` on your own username, so you can see the status of your jobs.

Let's take a look at the output. You can see some pretty useful info: the `JOBID`, what `PARTITION` the job is running under, and the `ST`atus. 

`ST`atus is really important, because it can tell you whether your job is:

`R` (Running), `PD` (Pending), `ST` (Stopped), `S` (Suspended), `CD` (Completed). 

There may be many reasons why your job is `PD` and isn't running. You might have asked for way more memory or CPUs that are available. The queue may be exceptionally full and you have been running a lot of jobs. `slurm` is [fair in how it allocates resources](https://accdoc.ohsu.edu/exacloud/guide/job-scheduler/), so that no one user can dominate the queue. Keep this in mind when you set up a job. Asking for 1 or 2 nodes (~24 to 48 CPUs) is pretty reasonable, but asking for 100 nodes might be asking for trouble. When in doubt, contact ACC.

```
            JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
           3970834 very_long KIRC.bla    wooma  R 11-04:21:09      1 exanode-7-15
           3970835 very_long OV.blast    wooma  R 11-04:20:32      1 exanode-7-5
           3970833 very_long GBM.blas    wooma  R 11-04:21:44      1 exanode-7-8
           4034201 very_long PAAD.bla    wooma  R 1-23:03:50      1 exanode-7-4
           4011215 very_long bacteria    wooma  R 4-04:23:28      1 exanode-7-10
           4002777 very_long viral_se    wooma  R 5-02:06:14      1 exanode-7-14
           3971962 very_long CESC.bla    wooma  R 11-02:10:12      1 exanode-0-26
           3970837 very_long BRCA.bla    wooma  R 11-04:19:07      1 exanode-2-3
           4011061 long_jobs SKCM_TCG    wooma  R 4-04:23:29      1 exanode-7-10
           4034206 long_jobs SKCM_TCG    wooma  R 1-23:01:51      1 exanode-2-20
           4044154 long_jobs process_    wooma  R    4:37:45      1 exanode-2-25
           4043015 long_jobs SKCM_TCG    wooma  R    7:14:20      1 exanode-2-44
```

## Task 2 - Let's use **srun**

Ok! Now we know the basic architecture of exacloud, let's use the foundational command of `srun`. Basically, `srun` will let you run a simple command on one of the compute nodes. We're going to run a python script with `srun`. We're not going to ask for anything special in terms of allocation; we'll take what `slurm` gives us.

If you only need to do a quick one off job, `srun` is your friend. If you need to do a job many times, you should consider using `sbatch` to automate it. 

We're going to run `samtools` on a small bam file. `samtools` is already installed on exacloud and is located here: 

`/opt/installed/samtools-1.6/bin/samtools`

1. `cd` into the `samtools_example` folder. We're going to run `samtools sort` on the first `bam` (`1.1-1000000.NA06984.454.MOSAIK.SRP000033.2009_11.bam`) file on here, and sort the reads. These are really small regions of this `bam` file.

```
cd samtools_example
```

2. Let's use `srun` to run `samtools`. Notice the command is identical to just running `samtools sort` but we just put an `srun` on it.

```
srun /opt/installed/samtools-1.6/bin/samtools sort 1.1-1000000.NA06984.454.MOSAIK.SRP000033.2009_11.bam -o 1.1-1000000.NA06984.454.MOSAIK.SRP000033.2009_11.sorted.bam
```
3. Let's index the sorted file file. 

```
srun /opt/installed/samtools-1.6/bin/samtools index 1.1-1000000.NA06984.454.MOSAIK.SRP000033.2009_11.sorted.bam
```

4. 

## Task 3 - Let's do an interactive job

Ok, let's look at an alternative way to do this. We may have to. We'll only request one CPU to do our job.

Before we run our interactive job, we're going to open a `screen` session. `screen` is a handy unix utility that lets you continue to run jobs even after you are disconnected from `exacloud`. I sometimes use it to keep an interactive session open so I can go back to it.

### Important Note - how to not get other `exacloud` users mad at you

*Please, please, please*, **don't run any jobs other than transferring files** on the head node of `exahead1`. Running something like an R job or a python job on it will slow down the head node, and make things run badly for everyone, since `exahead1` will be doing your job instead doing what it's there for, which is allocating resources for other jobs. In the worst case scenario, you will bring down `exahead1` and other users will be very, very, very upset with you.

How to avoid this? If you're running an interactive job, make sure you're not on the head node when you run it. Here's a quick way to check if you're on the head node. If you're on the head node, running `hostname` will return 

```exahead1``` 

If you're in an interactive session, `hostname` will return a different name, such as

```exanode-2-44.local.```

## Task 4 - Let's try a batch job

We're going to request 4 CPUs to run our batch job.

https://rcc.uchicago.edu/docs/running-jobs/array/index.html
https://www.rc.fas.harvard.edu/resources/documentation/submitting-large-numbers-of-jobs-to-odyssey/

# We have to go deeper

## Environment Variables are your friend

One of the big problems with running jobs on exacloud is that the compute nodes don't know about the location of software, or your library. 



# FAQS

## Slurm specific questions

https://slurm.schedmd.com/faq.html

## Exacloud specific questions

1) How do I know how much time to ask for?
2) How do I stay a good citizen on exacloud?
3) What is an appropriate use for lustre? How long can I leave data on there?
4) What is Research Data Storage and how do I get some?
5) How can I install my own versions of software?
6) Where should I keep personal libraries of software?

## Slurm Troubleshooting

https://slurm.schedmd.com/quickstart.html

## A little more knowledge

1) `tmux` or `screen` - open interactive jobs and keep them open
2) customizing your `.bashrc`
3) `rsync` - automate directory synchronization
4) Setting environment variables
5) installing specific libraries/software when you don't have root access
  + https://github.com/greenstick/bootstrapping-package-management-on-exacloud

## Be responsible

https://www.rc.fas.harvard.edu/resources/responsibilities/